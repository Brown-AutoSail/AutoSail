{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e1608fae-befb-4e26-84c8-98e4f09229cb",
   "metadata": {},
   "source": [
    "# 1. The Istio Configurator\n",
    "This class is responsible for applying different configurations to Istio.\n",
    "We are primarily investigating the destination rules of the istio configurations. \n",
    "We define 11 actions that our agent could take on each of the services. \n",
    "In specific, the SockShop is made up of 14 services. Therefore, there will be in total 11 x 14 = 154 actions. These actions define our action space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "8403b428-8cf3-46b7-b23a-965a6fc4943e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "import os\n",
    "\n",
    "config_file = \"destination-rules.yaml\"\n",
    "kubectl_apply_cmd = \"kubectl apply -f \"\n",
    "\n",
    "class IstioConfig:\n",
    "    def __init__(self, file_name):\n",
    "        self.file_name = file_name\n",
    "        with open(file_name, \"r\") as stream:\n",
    "            docs = yaml.load_all(stream, Loader=yaml.FullLoader)\n",
    "            self.docs = list(docs)\n",
    "    \n",
    "    # save to yaml file\n",
    "    def save(self):\n",
    "        with open(self.file_name, \"w\") as stream:\n",
    "            yaml.dump_all(self.docs, stream)\n",
    "    \n",
    "    # apply to Istio cluster\n",
    "    def apply(self):\n",
    "        self.save()\n",
    "        os.system(kubectl_apply_cmd + self.file_name + \" > /dev/null\")\n",
    "        \n",
    "    # reset to default config (does not save)\n",
    "    def reset(self):\n",
    "        for i in range(self.size()):\n",
    "            ic.docs[i][\"spec\"][\"trafficPolicy\"][\"connectionPool\"][\"http\"] = {}\n",
    "            ic.docs[i][\"spec\"][\"trafficPolicy\"][\"connectionPool\"][\"tcp\"] = {}\n",
    "    \n",
    "    def size(self):\n",
    "        return len(self.docs)\n",
    "    \n",
    "    def get_docs(self):\n",
    "        return self.docs\n",
    "    \n",
    "    def _get_http(self, i):\n",
    "        return ic.docs[i][\"spec\"][\"trafficPolicy\"][\"connectionPool\"][\"http\"]\n",
    "    \n",
    "    def _get_tcp(self, i):\n",
    "        return ic.docs[i][\"spec\"][\"trafficPolicy\"][\"connectionPool\"][\"tcp\"]\n",
    "    \n",
    "    #### tcp ####\n",
    "    # maxConnections: int32\n",
    "    def set_tcp_max_connections(self, i, val):\n",
    "        self._get_tcp(i)[\"maxConnections\"] = val\n",
    "    \n",
    "    # connectTimeout: int (unit ms)\n",
    "    def set_tcp_connect_timeout(self, i, val):\n",
    "        self._get_tcp(i)[\"connectTimeout\"] = str(val) + \"ms\"\n",
    "        \n",
    "    # TcpKeepalive time: int (unit ms)\n",
    "    def set_tcp_keeplive_probes(self, i, val):\n",
    "        val = str(val) + \"ms\"\n",
    "        tcp = self._get_tcp(i)\n",
    "        if \"tcpKeepalive\" in tcp:\n",
    "            tcp[\"tcpKeepalive\"][\"time\"] = val\n",
    "        else:\n",
    "            tcp[\"tcpKeepalive\"] = {\"time\": val}\n",
    "            \n",
    "    # TcpKeepalive time: int (unit ms)\n",
    "    def set_tcp_keeplive_time(self, i, val):\n",
    "        val = str(val) + \"ms\"\n",
    "        tcp = self._get_tcp(i)\n",
    "        if \"tcpKeepalive\" in tcp:\n",
    "            tcp[\"tcpKeepalive\"][\"time\"] = val\n",
    "        else:\n",
    "            tcp[\"tcpKeepalive\"] = {\"time\": val}\n",
    "            \n",
    "    # TcpKeepalive interval: int (unit ms)\n",
    "    def set_tcp_keeplive_interval(self, i, val):\n",
    "        val = str(val) + \"ms\"\n",
    "        tcp = self._get_tcp(i)\n",
    "        if \"tcpKeepalive\" in tcp:\n",
    "            tcp[\"tcpKeepalive\"][\"interval\"] = val\n",
    "        else:\n",
    "            tcp[\"tcpKeepalive\"] = {\"interval\": val}\n",
    "    \n",
    "    #### http ####\n",
    "    # http1MaxPendingRequests: int32\n",
    "    def set_http_http1_max_pending_requests(self, i, val):\n",
    "        self._get_http(i)[\"http1MaxPendingRequests\"] = val\n",
    "    \n",
    "    # http2MaxRequests: int32\n",
    "    def set_http_http2_max_requests(self, i, val):\n",
    "        self._get_http(i)[\"http2MaxRequests\"] = val\n",
    "    \n",
    "    # maxRequestsPerConnection: int32\n",
    "    def set_http_max_requests_per_connection(self, i, val):\n",
    "        self._get_http(i)[\"maxRequestsPerConnection\"] = val\n",
    "        \n",
    "    # maxRetries: int32\n",
    "    def set_http_max_retries(self, i, val):\n",
    "        self._get_http(i)[\"maxRetries\"] = val\n",
    "    \n",
    "    # idleTimeout : int (unit ms)\n",
    "    def set_http_idle_timeout(self, i, val):\n",
    "        self._get_http(i)[\"idleTimeout\"] = str(val) + \"ms\"\n",
    "        \n",
    "    # useClientProtocol: bool\n",
    "    def set_http_use_client_protocol(self, i, val):\n",
    "        self._get_http(i)[\"useClientProtocol\"] = val"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e84d0c6-4fc1-4c6b-abc0-1ee39fbe11e3",
   "metadata": {},
   "source": [
    "# 2. The Load Generator\n",
    "This class is responsible for generating load to the sockshop. After generating the load, it will output some metrics to the stdout. An example is shown below:  \n",
    "\n",
    "|Name                                                          |# reqs |  # fails |   Avg |   Min |    Max  |  Median | req/s|\n",
    "|:-------------------------------------------------------------|:------|:---------|:------|:------|:--------|:--------|:-----|\n",
    "|GET /                                                         |  110  |  0(0.00%)|    232|     38|    441  |     220 | 11.86|\n",
    "|GET /basket.html                                              |  113  |  0(0.00%)|    248|    119|    425  |     240 | 13.00|\n",
    "|DELETE /cart                                                  |  107  |  0(0.00%)|    400|    223|    584  |     390 | 12.00|\n",
    "|POST /cart                                                    |  103  |  5(4.63%)|    648|    312|    909  |     630 | 11.57|\n",
    "|GET /catalogue                                                |  113  |  0(0.00%)|    417|    202|    607  |     420 | 12.00|\n",
    "|GET /category.html                                            |  111  |  0(0.00%)|    230|     79|    443  |     220 | 12.29|\n",
    "|GET /detail.html?id=03fef6ac-1896-4ce8-bd69-b798f85c6e0b      |   12  |  0(0.00%)|    215|    116|    300  |     200 |  1.29|\n",
    "|GET /detail.html?id=3395a43e-2d88-40de-b95f-e00e1502085b      |   10  |  0(0.00%)|    205|    111|    313  |     200 |  1.29|\n",
    "|GET /detail.html?id=510a0d7e-8e83-4193-b483-e27e09ddc34d      |   17  |  0(0.00%)|    235|    146|    396  |     220 |  1.57|\n",
    "|GET /detail.html?id=808a2de1-1aaa-4c25-a9b9-6612e8f29a38      |   15  |  0(0.00%)|    206|    120|    303  |     210 |  1.71|\n",
    "|GET /detail.html?id=819e1fbf-8b7e-4f6d-811f-693534916a8b      |    9  |  0(0.00%)|    215|    126|    331  |     200 |  1.14|\n",
    "|GET /detail.html?id=837ab141-399e-4c1f-9abc-bace40296bac      |   11  |  0(0.00%)|    217|    114|    351  |     180 |  1.00|\n",
    "|GET /detail.html?id=a0a4f044-b040-410d-8ead-4de0446aec7e      |   10  |  0(0.00%)|    223|    174|    335  |     200 |  1.43|\n",
    "|GET /detail.html?id=d3588630-ad8e-49df-bbd7-3167f7efb246      |   11  |  0(0.00%)|    227|    144|    407  |     200 |  0.71|\n",
    "|GET /detail.html?id=zzz4f044-b040-410d-8ead-4de0446aec7e      |   17  |  0(0.00%)|    211|    115|    365  |     210 |  1.86|\n",
    "|GET /login                                                    |  116  |  0(0.00%)|    654|    450|    898  |     620 | 12.14|\n",
    "|POST /orders                                                  |  101  |  9(8.18%)|    829|    572|   1153  |     820 | 10.71|\n",
    "-------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "We are using these metrics to define the current state of the environment. Therefore, each state will have 17 * 8 = 136 values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7eb646de-da3b-43d6-9316-f79c3ed3e47d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "\n",
    "GATEWAY_URL = '34.133.80.64:80'\n",
    "\n",
    "#def generate_load(num_clients, num_reqs):\n",
    "#cmd = \"docker run load-test -h {url} -r {num_reqs} -c {num_clients}\".format(url = GATEWAY_URL, num_reqs = num_reqs, num_clients = num_clients)\n",
    "#return subprocess.getoutput(cmd)\n",
    "\n",
    "#output = subprocess.getoutput(cmd);\n",
    "\n",
    "class load_generator:\n",
    "    def __init__(self, gateway_url):\n",
    "        self.gateway_url = gateway_url\n",
    "    \n",
    "    def generate_load(self, num_clients, num_reqs):\n",
    "        cmd = \"docker run load-test -h {url} -r {num_reqs} -c {num_clients}\".format(url = self.gateway_url, num_reqs = num_reqs, num_clients = num_clients)\n",
    "        return subprocess.getoutput(cmd)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2a212eb-73c9-4dec-82db-538d1d23d5c7",
   "metadata": {},
   "source": [
    "# 3. Parser\n",
    "After we generate the load and get the output, we use regular expression to extract the data from the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8cdd3f04-2db5-4745-b1ff-0b3473e9bd5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def parse(result):\n",
    "    \"\"\"\n",
    "    input: os system result\n",
    "    output: two dictionary of data\n",
    "    \n",
    "    \"\"\"\n",
    "    delimiter = \"--------------------------------------------------------------------------------------------------------------------------------------------\"\n",
    "    data = result.split(delimiter)\n",
    "\n",
    "    # create dictionary of request data\n",
    "    request_data = data[1]\n",
    "    request_data_key, request_data_val = extract(request_data, 7)\n",
    "    request_data_dict = dict(zip(request_data_key, request_data_val))\n",
    "\n",
    "    # create dictionary of request data\n",
    "    request_complete_percent = data[3]\n",
    "    request_complete_percent_key, request_complete_percent_val = extract(request_complete_percent, 10)\n",
    "    request_complete_percent_dict = dict(zip(request_complete_percent_key, request_complete_percent_val))    \n",
    "    return request_data_dict, request_complete_percent_dict\n",
    "\n",
    "\n",
    "def extract(data, num_col):\n",
    "    # select all values\n",
    "    regex_val = \"([0-9]+\\([0-9]+\\.[0-9]+\\%\\)|[0-9]+\\.[0-9]+| [0-9]+ )\"\n",
    "    val = re.findall(regex_val, data)\n",
    "\n",
    "\n",
    "    # remove space\n",
    "    val = [x.strip(' ') for x in val]\n",
    "\n",
    "    # group each row into nested list\n",
    "    val = [val[i:i + num_col] for i in range(0, len(val), num_col)]\n",
    "    \n",
    "    # select all keys\n",
    "    regex_key = \"[A-Z]+ [^\\s]+\"\n",
    "    key = re.findall(regex_key, data)\n",
    "    return key, val\n",
    "\n",
    "def get_load_metrics(request_dict):\n",
    "    failure_rates = []\n",
    "    rps = []\n",
    "    for key in request_dict.keys():\n",
    "        metrics = request_dict[key]\n",
    "        temp = metrics[1]\n",
    "        failure_rates.append(float(temp[temp.index('(')+1:temp.index('%')]))\n",
    "        rps.append(float(metrics[6]))\n",
    "    return failure_rates, rps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec239023-edf9-4489-b487-ca1289710268",
   "metadata": {},
   "outputs": [],
   "source": [
    "# An example\n",
    "GATEWAY_URL = '34.133.80.64:80'\n",
    "generator = load_generator(GATEWAY_URL)\n",
    "request_data_dict, request_complete_percent_dict = parse(generator.generate_load(50,1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1e23355-95d6-456a-becf-3404753ea11d",
   "metadata": {},
   "outputs": [],
   "source": [
    "request_data_dict, request_complete_percent_dict = parse(generator.generate_load(5,100))\n",
    "request_data_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f28fecd3-fdaf-4778-a7a3-31ec58911523",
   "metadata": {},
   "source": [
    "# 3. The Environment\n",
    "We are using a reinforcement learning agent to tune the configuration of istio. \n",
    "We therefore need to provide an environment for the agent to play with. \n",
    "Our Environment is comprised of a load generator and a istio configurator. \n",
    "The states of the environment will be the failure rates, rps, of all the SockShop endpoints in combine. \n",
    "The actions our agent could take are provided by the istio configurator. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "4c08568b-402e-43eb-a4fc-aa53d6289351",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_fail():\n",
    "    failure_rates = [100.0] * 17\n",
    "    latencies = [0.1] * 4\n",
    "    return failure_rates, None, latencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "9ea9eb9a-aeb0-4281-9afd-596d8292691b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "def check_fail():\n",
    "    GATEWAY_URL = 'http://34.133.80.64:80/catalogue'\n",
    "    data = requests.get(GATEWAY_URL).status_code\n",
    "    return data != 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "02a463cf-fbf3-4b28-939f-82cf5a99f137",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "check_fail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "id": "a3f352e7-d100-43f7-a44e-a3d5e9c59cfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import time\n",
    "from random import sample\n",
    "\n",
    "class Env:\n",
    "    \n",
    "    def __init__(self, istio_configurator, load_generator, prometheus_client):\n",
    "        '''\n",
    "            The Environment class should take an istio configurator, and a load\n",
    "            generator. This will be the environment our RL agent interacts with.\n",
    "            \n",
    "            The states are defined by the outputs of the load generators, which\n",
    "            have the rps, todtal requests, failure rates of each endpoints in\n",
    "            the SockShop.\n",
    "            \n",
    "            The actions are defined by the istio parameters, specifically by the \n",
    "            istio connection pool settings in the Destination Rules.\n",
    "        '''\n",
    "        self.istio_configurator = istio_configurator\n",
    "        self.load_generator = load_generator\n",
    "        self.prometheus_client = prometheus_client\n",
    "        \n",
    "        NUM_CLIENTS_UPPER_BOUND = 20\n",
    "        \n",
    "        self.num_clients = random.randint(10, NUM_CLIENTS_UPPER_BOUND)\n",
    "        self.num_reqs = 80 * self.num_clients\n",
    "        \n",
    "        self.init_config = None\n",
    "        \n",
    "    def __make_init_config(self):\n",
    "        print(\"making new config\")\n",
    "        \n",
    "        UPPER_BOUND = 100 # can change this\n",
    "        NUM_ENDPOINTS = 14\n",
    "        NUM_ACTIONS = 6\n",
    "        \n",
    "        for i in range(NUM_ENDPOINTS):\n",
    "            if i != 4:\n",
    "                # act = random.randint(0, NUM_ACTIONS-1)\n",
    "                val = random.randint(1, UPPER_BOUND)\n",
    "                self.istio_configurator.set_tcp_max_connections(i, val)\n",
    "                val = random.randint(1, UPPER_BOUND)\n",
    "                self.istio_configurator.set_tcp_connect_timeout(i, val)\n",
    "                val = random.randint(1, UPPER_BOUND)\n",
    "                self.istio_configurator.set_http_http1_max_pending_requests(i, val)\n",
    "                val = random.randint(1, UPPER_BOUND)\n",
    "                self.istio_configurator.set_http_http2_max_requests(i, val)\n",
    "                val = random.randint(1, UPPER_BOUND)\n",
    "                self.istio_configurator.set_http_max_requests_per_connection(i, val)\n",
    "                val = random.randint(1, UPPER_BOUND)\n",
    "                self.istio_configurator.set_http_max_retries(i, val)\n",
    "        self.istio_configurator.apply()\n",
    "        if check_fail():\n",
    "            print(\"Response code is not 200, makeing new config\")\n",
    "            return self.__make_init_config()\n",
    "        print(\"num clients {}, num reqs {}\".format(self.num_clients, self.num_reqs))\n",
    "        output = self.load_generator.generate_load(self.num_clients, self.num_reqs)\n",
    "        dic1, _ = parse(output)\n",
    "        \n",
    "        failure_rates, rps = get_load_metrics(dic1)\n",
    "        if sum(rps) < 1:\n",
    "            print(\"Somehow get all 0s, repeating\")\n",
    "            return self.__make_init_config()\n",
    "        \n",
    "        self.init_config = self.istio_configurator.get_docs().copy()\n",
    "    \n",
    "    def __reset_init_config(self):\n",
    "        print(\"resetting config\")\n",
    "        self.istio_configurator.docs = self.init_config.copy()\n",
    "        self.istio_configurator.apply()\n",
    "        \n",
    "    \n",
    "    def generate_initial_state(self):\n",
    "        '''\n",
    "            restore to the a fixed initial istio configurations,\n",
    "            generate random load.\n",
    "            \n",
    "            return the failure rates, rps, 99 quantile latencies\n",
    "        '''\n",
    "\n",
    "# commented out code for random initial state\n",
    "#         UPPER_BOUND = 100 # can change this\n",
    "#         NUM_ENDPOINTS = 14\n",
    "#         NUM_ACTIONS = 6\n",
    "        \n",
    "#         # randomly config the istio\n",
    "#         self.istio_configurator.reset()\n",
    "        \n",
    "#         for i in range(NUM_ENDPOINTS):\n",
    "#             if i != 4:\n",
    "#                 # act = random.randint(0, NUM_ACTIONS-1)\n",
    "#                 val = random.randint(1, UPPER_BOUND)\n",
    "#                 self.istio_configurator.set_tcp_max_connections(i, val)\n",
    "#                 val = random.randint(1, UPPER_BOUND)\n",
    "#                 self.istio_configurator.set_tcp_connect_timeout(i, val)\n",
    "#                 val = random.randint(1, UPPER_BOUND)\n",
    "#                 self.istio_configurator.set_http_http1_max_pending_requests(i, val)\n",
    "#                 val = random.randint(1, UPPER_BOUND)\n",
    "#                 self.istio_configurator.set_http_http2_max_requests(i, val)\n",
    "#                 val = random.randint(1, UPPER_BOUND)\n",
    "#                 self.istio_configurator.set_http_max_requests_per_connection(i, val)\n",
    "#                 val = random.randint(1, UPPER_BOUND)\n",
    "#                 self.istio_configurator.set_http_max_retries(i, val)\n",
    "\n",
    "#         self.istio_configurator.apply()\n",
    "        \n",
    "#         if check_fail():\n",
    "#             print(\"Response code is not 200\")\n",
    "#             return get_fail()\n",
    "        \n",
    "#         # generate load\n",
    "#         print(\"num clients {}, num reqs {}\".format(self.num_clients, self.num_reqs))\n",
    "        \n",
    "#         output = self.load_generator.generate_load(self.num_clients, self.num_reqs)\n",
    "#         #print(output)\n",
    "#         dic1, _ = parse(output)\n",
    "        \n",
    "#         failure_rates, rps = get_load_metrics(dic1)\n",
    "#         while sum(rps) < 1:\n",
    "#             print(\"Somehow get all 0s, repeating\")\n",
    "#             # randomly config the istio\n",
    "#             self.istio_configurator.reset()\n",
    "            \n",
    "#             for i in range(NUM_ENDPOINTS):\n",
    "#                 if i != 4:\n",
    "#                     val = random.randint(1, UPPER_BOUND)\n",
    "#                     self.istio_configurator.set_tcp_max_connections(i, val)\n",
    "#                     val = random.randint(1, UPPER_BOUND)\n",
    "#                     self.istio_configurator.set_tcp_connect_timeout(i, val)\n",
    "#                     val = random.randint(1, UPPER_BOUND)\n",
    "#                     self.istio_configurator.set_http_http1_max_pending_requests(i, val)\n",
    "#                     val = random.randint(1, UPPER_BOUND)\n",
    "#                     self.istio_configurator.set_http_http2_max_requests(i, val)\n",
    "#                     val = random.randint(1, UPPER_BOUND)\n",
    "#                     self.istio_configurator.set_http_max_requests_per_connection(i, val)\n",
    "#                     val = random.randint(1, UPPER_BOUND)\n",
    "#                     self.istio_configurator.set_http_max_retries(i, val)\n",
    "\n",
    "#             self.istio_configurator.apply()\n",
    "            \n",
    "#             #check\n",
    "#             if check_fail():\n",
    "#                 print(\"Response code is not 200\")\n",
    "#                 return get_fail()\n",
    "            \n",
    "#             output = self.load_generator.generate_load(self.num_clients, self.num_reqs)\n",
    "#             dic1, _ = parse(output)\n",
    "#             failure_rates, rps = get_load_metrics(dic1)\n",
    "\n",
    "        if not self.init_config:\n",
    "            self.__make_init_config()\n",
    "        else:\n",
    "            self.__reset_init_config()\n",
    "    \n",
    "        time.sleep(3)\n",
    "        metrics = self.prometheus_client.custom_query(query=\"microservices_demo_user_request_latency_microseconds\")\n",
    "        if len(metrics) > 0:\n",
    "            latencies_99_quantile = list([float(metrics[i]['value'][1]) for i in [2,5,8,14]])\n",
    "        else:\n",
    "            latencies_99_quantile = [100, 100, 100, 100]\n",
    "        \n",
    "        return failure_rates, rps, latencies_99_quantile\n",
    "        \n",
    "    def apply_action(self, action):\n",
    "        '''\n",
    "            action = #i * NUM_ENDPOINTS + #act\n",
    "        '''\n",
    "        NUM_ENDPOINTS = 13\n",
    "        i = int(action) // NUM_ENDPOINTS\n",
    "        if i == 4:\n",
    "            i += 1\n",
    "        act = int(action) % NUM_ENDPOINTS\n",
    "        \n",
    "        if act == 0:\n",
    "            self.istio_configurator.set_tcp_max_connections(i, self.istio_configurator._get_tcp(i)[\"maxConnections\"] + 20)\n",
    "        elif act == 1:\n",
    "            self.istio_configurator.set_tcp_max_connections(i, max(1, self.istio_configurator._get_tcp(i)[\"maxConnections\"] - 5))\n",
    "        elif act == 2:\n",
    "            self.istio_configurator.set_http_http1_max_pending_requests(i, self.istio_configurator._get_http(i)[\"http1MaxPendingRequests\"] + 40)\n",
    "        elif act == 3:\n",
    "            self.istio_configurator.set_http_http1_max_pending_requests(i, max(1, self.istio_configurator._get_http(i)[\"http1MaxPendingRequests\"] - 10))\n",
    "        elif act == 4:\n",
    "            self.istio_configurator.set_http_max_requests_per_connection(i, self.istio_configurator._get_http(i)[\"maxRequestsPerConnection\"] + 40)\n",
    "        elif act == 5:\n",
    "            self.istio_configurator.set_http_max_requests_per_connection(i, max(1, self.istio_configurator._get_http(i)[\"maxRequestsPerConnection\"] - 10))\n",
    "        elif act == 6:\n",
    "            curr = self.istio_configurator._get_tcp(i)[\"connectTimeout\"]\n",
    "            curr_val = int(curr[:curr.index('m')])\n",
    "            self.istio_configurator.set_tcp_connect_timeout(i, curr_val + 200)\n",
    "        elif act == 7:\n",
    "            curr = self.istio_configurator._get_tcp(i)[\"connectTimeout\"]\n",
    "            curr_val = int(curr[:curr.index('m')])\n",
    "            self.istio_configurator.set_tcp_connect_timeout(i, max(1, curr_val - 50))\n",
    "        elif act == 8:\n",
    "            self.istio_configurator.set_http_http2_max_requests(i, self.istio_configurator._get_http(i)[\"http2MaxRequests\"] + 200)\n",
    "        elif act == 9:\n",
    "            self.istio_configurator.set_http_http2_max_requests(i, max(1,self.istio_configurator._get_http(i)[\"http2MaxRequests\"] - 50))\n",
    "        elif act == 10:\n",
    "            self.istio_configurator.set_http_max_retries(i, self.istio_configurator._get_http(i)[\"maxRetries\"] + 40)\n",
    "        else:\n",
    "            self.istio_configurator.set_http_max_retries(i, max(1, self.istio_configurator._get_http(i)[\"maxRetries\"] - 10))\n",
    "\n",
    "        self.istio_configurator.apply()\n",
    "        time.sleep(3)\n",
    "        \n",
    "        if check_fail():\n",
    "            return get_fail()\n",
    "        \n",
    "        # generate same load\n",
    "        output = self.load_generator.generate_load(self.num_clients, self.num_reqs)\n",
    "        #print(output)\n",
    "        dic1, _ = parse(output)\n",
    "        \n",
    "        failure_rates, rps = get_load_metrics(dic1)\n",
    "        while sum(rps) < 1:\n",
    "            print(\"Somehow get all 0s, repeating\")\n",
    "            if check_fail():\n",
    "                return get_fail()\n",
    "            output = self.load_generator.generate_load(self.num_clients, self.num_reqs)\n",
    "            dic1, _ = parse(output)\n",
    "            failure_rates, rps = get_load_metrics(dic1)\n",
    "            \n",
    "        time.sleep(3)\n",
    "        metrics = self.prometheus_client.custom_query(query=\"microservices_demo_user_request_latency_microseconds\")\n",
    "        if len(metrics) > 0:\n",
    "            latencies_99_quantile = list([float(metrics[i]['value'][1]) for i in [2,5,8,14]])\n",
    "        else:\n",
    "            latencies_99_quantile = [100, 100, 100, 100]\n",
    "        \n",
    "        return failure_rates, rps, latencies_99_quantile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "2b3b2944-4274-49c4-b3bd-f1e29df51c10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test env\n",
    "\n",
    "ic = IstioConfig(config_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "id": "8d4b52e8-b21b-4991-9c75-1ac9dc191fbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://localhost:9090\n"
     ]
    }
   ],
   "source": [
    "from subprocess import Popen\n",
    "from prometheus_api_client import PrometheusConnect\n",
    "\n",
    "prometheus_host_cmd = [\"/home/jupyter/.istioctl/bin/istioctl\", \"dashboard\", \"prometheus\"]\n",
    "prometheus_host = \"http://localhost:9090\"\n",
    "\n",
    "# start a local Prometheus host\n",
    "host = subprocess.Popen(prometheus_host_cmd)\n",
    "client = PrometheusConnect(url =prometheus_host, disable_ssl=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 454,
   "id": "83bcec38-c20c-445f-acfd-f99ac3d2afa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# kill prometheus host in case of erro\n",
    "# host.kill()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 455,
   "id": "19ad6df4-815c-4e7c-aeb7-9a6df66f7c47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a Env\n",
    "env = Env(ic, generator, client)\n",
    "#failure_rates, rps, latencies = env.generate_initial_state()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccd7293d-03b5-4663-ae80-f61afce22b6b",
   "metadata": {},
   "source": [
    "# 4. The Reinforcement Learning Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fe5d8001-0520-4cc0-aafc-9c7038b69e56",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "50928eca-1917-49bb-b124-51bd34460dcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "\n",
    "class Agent(tf.keras.Model):\n",
    "    def __init__(self, num_actions):\n",
    "        '''\n",
    "            initialization of the model\n",
    "            use simple dense layers\n",
    "        '''\n",
    "        super(Agent, self).__init__()\n",
    "        self.num_actions = num_actions\n",
    "        self.network = keras.Sequential([\n",
    "            keras.layers.Dense(units=250, activation='relu'),\n",
    "            keras.layers.Dropout(0.2),\n",
    "            keras.layers.Dense(units=num_actions, activation='softmax')\n",
    "        ])\n",
    "        self.rewards = []\n",
    "    \n",
    "    def make_action(self, state):\n",
    "        '''\n",
    "            return the probabilities of taking each action\n",
    "        '''\n",
    "        return self.network(state)\n",
    "    \n",
    "    def loss(self, states, discounted_rewards, actions):\n",
    "        prbs = self.make_action(states)\n",
    "        prbs_actions = tf.gather_nd(prbs, np.expand_dims(actions, axis=1), batch_dims=1)\n",
    "        loss = -tf.reduce_sum(tf.multiply(tf.math.log(prbs_actions), discounted_rewards))\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "126e5fea-768f-4769-98ca-c922492c3757",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_ENDPOINTS = 13\n",
    "NUM_ACTIONS = 12\n",
    "TOTAL_NUM_ACTIONS = NUM_ENDPOINTS * NUM_ACTIONS\n",
    "agent = Agent(TOTAL_NUM_ACTIONS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a696dd9f-2196-409c-b5d2-75fb85b9429e",
   "metadata": {},
   "source": [
    "# 5. Main - Putting Everything Together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "87e6ca5a-3b58-4004-a2d3-70bb59aeb2b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_ENDPOINTS = 13\n",
    "NUM_ACTIONS = 12\n",
    "TOTAL_NUM_ACTIONS = NUM_ENDPOINTS * NUM_ACTIONS\n",
    "\n",
    "\n",
    "def reward_function(self):\n",
    "    discounted_rewards = [0] * len(self.rewards) # init discount reward\n",
    "    for i in reversed(range(len(self.rewards))):\n",
    "        if i == len(self.rewards) - 1:\n",
    "            discounted_rewards[i] = self.rewards[i]\n",
    "        else:\n",
    "            discounted_rewards[i] = self.rewards[i] + discount_factor * discounted_rewards[i + 1]\n",
    "    self.rewards = discounted_rewards\n",
    "    return \n",
    "    \n",
    "def evaluate(fails, latencies):\n",
    "    return -(sum(fails) + sum(latencies) * 10000)\n",
    "\n",
    "def generate_episode(num_trials):\n",
    "    states = []\n",
    "    actions = []\n",
    "    rewards = []\n",
    "    \n",
    "    failure_rates, rps, latencies = env.generate_initial_state()\n",
    "    state = tf.convert_to_tensor(failure_rates + latencies)\n",
    "    prev_eval = evaluate(failure_rates, latencies)\n",
    "    \n",
    "    for i in range(num_trials):\n",
    "        print(\"current trial is: {}\".format(i+1))\n",
    "        prbs_actions = agent.make_action(np.expand_dims(state, axis=0))\n",
    "        prbs = np.reshape(prbs_actions, (agent.num_actions))\n",
    "        action = np.random.choice(np.arange(agent.num_actions), p=prbs)\n",
    "        states.append(state)\n",
    "        actions.append(action)\n",
    "        f_rates, _, latencies = env.apply_action(action)\n",
    "        state = tf.convert_to_tensor(f_rates + latencies)\n",
    "        curr_eval = evaluate(f_rates, latencies)\n",
    "        rewards.append(curr_eval - prev_eval)\n",
    "        prev_eval = curr_eval\n",
    "    \n",
    "    return states, actions, rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "12118202-5666-4ea8-8b57-2f11dd5b6f84",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(env, agent):\n",
    "    with tf.GradientTape() as tape:\n",
    "        states, actions, rewards = generate_episode(20)\n",
    "        loss = agent.loss(tf.convert_to_tensor(states),\n",
    "                       tf.convert_to_tensor(rewards),\n",
    "                       tf.convert_to_tensor(actions))\n",
    "    gradients = tape.gradient(loss, agent.trainable_variables)\n",
    "    tf.keras.optimizers.Adam(learning_rate=1e-2).apply_gradients(zip(gradients, agent.trainable_variables))\n",
    "    print(\"rewards are\", rewards)\n",
    "    return np.sum(rewards), states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ee8392f-f3eb-464c-871d-3e21fc393698",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run this block to start training\n",
    "epochs = 70\n",
    "rewards = []\n",
    "all_states = []\n",
    "for i in range(epochs):\n",
    "    start_time = time.time()\n",
    "    print(\"------------------ epoch {} ------------------------\".format(i+1))\n",
    "    reward, states = train(env, agent)\n",
    "    rewards.append(reward)\n",
    "    all_states.append(states)\n",
    "    print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f637ce4-8800-4e9d-a9df-e91c4d6a8a9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_states, rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bee8b68-052d-4502-9bc5-45d1bc4a32e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# optionally dump all_states and reward to file\n",
    "import pickle\n",
    "with open('new_rewards.list', 'wb') as file:\n",
    "    pickle.dump(rewards, file)\n",
    "with open('new_all_states.list', 'wb') as file:\n",
    "    pickle.dump(all_states, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 457,
   "id": "8f588ded-c997-4044-a5f3-7dd27eaa0e06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is how you read them\n",
    "# with open('rewards.list', 'rb') as file:\n",
    "#     rewards = pickle.load(file)\n",
    "    \n",
    "# with open('all_states.list', 'rb') as file:\n",
    "#      = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4da382fd-2097-48a9-b743-c38ad61f7c57",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "tf2-gpu.2-6.m86",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-6:m86"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
